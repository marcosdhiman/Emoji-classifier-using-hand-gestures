{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE THE GESTURE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "image_x, image_y = 50, 50\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "fbag = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "def main(g_id):\n",
    "    total_pics = 1200\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    x, y, w, h = 300, 50, 350, 350\n",
    "\n",
    "    create_folder(\"gestures/\" + str(g_id))\n",
    "    pic_no = 0\n",
    "    flag_start_capturing = False\n",
    "    frames = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        mask2 = cv2.inRange(hsv, np.array([2, 50, 60]), np.array([25, 150, 255]))\n",
    "        res = cv2.bitwise_and(frame, frame, mask=mask2)\n",
    "        gray = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
    "        median = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        kernel_square = np.ones((5, 5), np.uint8)\n",
    "        dilation = cv2.dilate(median, kernel_square, iterations=2)\n",
    "        opening=cv2.morphologyEx(dilation,cv2.MORPH_CLOSE,kernel_square)\n",
    "\n",
    "        ret, thresh = cv2.threshold(opening, 30, 255, cv2.THRESH_BINARY)\n",
    "        thresh = thresh[y:y + h, x:x + w]\n",
    "        contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[1]\n",
    "\n",
    "        if len(contours) > 0:\n",
    "            contour = max(contours, key=cv2.contourArea)\n",
    "            if cv2.contourArea(contour) > 10000 and frames > 50:\n",
    "                x1, y1, w1, h1 = cv2.boundingRect(contour)\n",
    "                pic_no += 1\n",
    "                save_img = thresh[y1:y1 + h1, x1:x1 + w1]\n",
    "                if w1 > h1:\n",
    "                    save_img = cv2.copyMakeBorder(save_img, int((w1 - h1) / 2), int((w1 - h1) / 2), 0, 0,\n",
    "                                                  cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "                elif h1 > w1:\n",
    "                    save_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1 - w1) / 2), int((h1 - w1) / 2),\n",
    "                                                  cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "                save_img = cv2.resize(save_img, (image_x, image_y))\n",
    "                cv2.putText(frame, \"Capturing...\", (30, 60), cv2.FONT_HERSHEY_TRIPLEX, 2, (127, 255, 255))\n",
    "                cv2.imwrite(\"gestures/\" + str(g_id) + \"/\" + str(pic_no) + \".jpg\", save_img)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, str(pic_no), (30, 400), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (127, 127, 255))\n",
    "        cv2.imshow(\"Capturing gesture\", frame)\n",
    "        cv2.imshow(\"thresh\", thresh)\n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('c'):\n",
    "            if flag_start_capturing == False:\n",
    "                flag_start_capturing = True\n",
    "            else:\n",
    "                flag_start_capturing = False\n",
    "                frames = 0\n",
    "        if flag_start_capturing == True:\n",
    "            frames += 1\n",
    "        if pic_no == total_pics:\n",
    "            break\n",
    "\n",
    "\n",
    "g_id = input(\"Enter gesture number: \")\n",
    "main(g_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERT THE IMAGES INTO A CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "root = './gestures' \n",
    "\n",
    "# go through each directory in the root folder given above\n",
    "for directory, subdirectories, files in os.walk(root):\n",
    "# go through each file in that directory\n",
    "    for file in files:\n",
    "    # read the image file and extract its pixels\n",
    "        print(file)\n",
    "        im = imread(os.path.join(directory,file))\n",
    "        value = im.flatten()\n",
    "\n",
    "\n",
    "        value = np.hstack((directory[11:],value))\n",
    "        df = pd.DataFrame(value).T\n",
    "        df = df.sample(frac=1) # shuffle the dataset\n",
    "        with open('train.csv', 'a') as dataset:\n",
    "            df.to_csv(dataset, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def keras_model(image_x, image_y):\n",
    "    num_of_classes = 12\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(image_x, image_y, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='sigmoid'))\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    filepath = \"emojinator.h5\"\n",
    "    checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint1]\n",
    "\n",
    "    return model, callbacks_list\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(\"train_foo.csv\")\n",
    "    dataset = np.array(data)\n",
    "    np.random.shuffle(dataset)\n",
    "    X = dataset\n",
    "    Y = dataset\n",
    "    X = X[:, 1:2501]\n",
    "    Y = Y[:, 0]\n",
    "\n",
    "    X_train = X[0:12000, :]\n",
    "    X_train = X_train / 255.\n",
    "    X_test = X[12000:13201, :]\n",
    "    X_test = X_test / 255.\n",
    "\n",
    "    # Reshape\n",
    "    Y = Y.reshape(Y.shape[0], 1)\n",
    "    Y_train = Y[0:12000, :]\n",
    "    Y_train = Y_train.T\n",
    "    Y_test = Y[12000:13201, :]\n",
    "    Y_test = Y_test.T\n",
    "\n",
    "    print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "    print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "    print(\"X_train shape: \" + str(X_train.shape))\n",
    "    print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "    print(\"X_test shape: \" + str(X_test.shape))\n",
    "    print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "    image_x = 50\n",
    "    image_y = 50\n",
    "\n",
    "    train_y = np_utils.to_categorical(Y_train)\n",
    "    test_y = np_utils.to_categorical(Y_test)\n",
    "    train_y = train_y.reshape(train_y.shape[1], train_y.shape[2])\n",
    "    test_y = test_y.reshape(test_y.shape[1], test_y.shape[2])\n",
    "    X_train = X_train.reshape(X_train.shape[0], 50, 50, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 50, 50, 1)\n",
    "    print(\"X_train shape: \" + str(X_train.shape))\n",
    "    print(\"X_test shape: \" + str(X_test.shape))\n",
    "\n",
    "    model, callbacks_list = keras_model(image_x, image_y)\n",
    "    model.fit(X_train, train_y, validation_data=(X_test, test_y), epochs=10, batch_size=64,\n",
    "              callbacks=callbacks_list)\n",
    "    scores = model.evaluate(X_test, test_y, verbose=0)\n",
    "    print(\"CNN Error: %.2f%%\" % (100 - scores[1] * 100))\n",
    "\n",
    "    model.save('emojinator.h5')\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE MODEL USING WEBCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = load_model('emojinator.h5')\n",
    "\n",
    "def main():\n",
    "    emojis = get_emojis()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    x, y, w, h = 300, 50, 350, 350\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret, img = cap.read()\n",
    "        img = cv2.flip(img, 1)\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        mask2 = cv2.inRange(hsv, np.array([2, 50, 60]), np.array([25, 150, 255]))\n",
    "        res = cv2.bitwise_and(img, img, mask=mask2)\n",
    "        gray = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
    "        median = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        kernel_square = np.ones((5, 5), np.uint8)\n",
    "        dilation = cv2.dilate(median, kernel_square, iterations=2)\n",
    "        opening = cv2.morphologyEx(dilation, cv2.MORPH_CLOSE, kernel_square)\n",
    "        ret, thresh = cv2.threshold(opening, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        thresh = thresh[y:y + h, x:x + w]\n",
    "        contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[1]\n",
    "        if len(contours) > 0:\n",
    "            contour = max(contours, key=cv2.contourArea)\n",
    "            if cv2.contourArea(contour) > 2500:\n",
    "                x, y, w1, h1 = cv2.boundingRect(contour)\n",
    "                newImage = thresh[y:y + h1, x:x + w1]\n",
    "                newImage = cv2.resize(newImage, (50, 50))\n",
    "                pred_probab, pred_class = keras_predict(model, newImage)\n",
    "                print(pred_class, pred_probab)\n",
    "                img = overlay(img, emojis[pred_class], 400, 250, 90, 90)\n",
    "\n",
    "        x, y, w, h = 300, 50, 350, 350\n",
    "        cv2.imshow(\"Frame\", img)\n",
    "        cv2.imshow(\"Contours\", thresh)\n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "def keras_predict(model, image):\n",
    "    processed = keras_process_image(image)\n",
    "    pred_probab = model.predict(processed)[0]\n",
    "    pred_class = list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab), pred_class\n",
    "\n",
    "\n",
    "def keras_process_image(img):\n",
    "    image_x = 50\n",
    "    image_y = 50\n",
    "    img = cv2.resize(img, (image_x, image_y))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.reshape(img, (-1, image_x, image_y, 1))\n",
    "    return img\n",
    "\n",
    "def get_emojis():\n",
    "    emojis_folder = 'hand_emo/'\n",
    "    emojis = []\n",
    "    for emoji in range(len(os.listdir(emojis_folder))):\n",
    "        print(emoji)\n",
    "        emojis.append(cv2.imread(emojis_folder+str(emoji)+'.png', -1))\n",
    "    return emojis\n",
    "\n",
    "def overlay(image, emoji, x,y,w,h):\n",
    "    emoji = cv2.resize(emoji, (w, h))\n",
    "    try:\n",
    "        image[y:y+h, x:x+w] = blend_transparent(image[y:y+h, x:x+w], emoji)\n",
    "    except:\n",
    "        pass\n",
    "    return image\n",
    "\n",
    "def blend_transparent(face_img, overlay_t_img):\n",
    "    # Split out the transparency mask from the colour info\n",
    "    overlay_img = overlay_t_img[:,:,:3] # Grab the BRG planes\n",
    "    overlay_mask = overlay_t_img[:,:,3:]  # And the alpha plane\n",
    "\n",
    "    # Again calculate the inverse mask\n",
    "    background_mask = 255 - overlay_mask\n",
    "\n",
    "    # Turn the masks into three channel, so we can use them as weights\n",
    "    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)\n",
    "    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Create a masked out face image, and masked out overlay\n",
    "    # We convert the images to floating point in range 0.0 - 1.0\n",
    "    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))\n",
    "    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))\n",
    "\n",
    "    # And finally just add them together, and rescale it back to an 8bit integer image\n",
    "    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))\n",
    "\n",
    "keras_predict(model, np.zeros((50, 50, 1), dtype=np.uint8))\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
